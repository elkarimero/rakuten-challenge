{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09cd0f7b-2f22-4289-ad55-1d776f250721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-21 16:22:29.368990: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742570549.553572    5577 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742570549.608568    5577 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742570549.996795    5577 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742570549.996854    5577 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742570549.996856    5577 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742570549.996857    5577 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-21 16:22:30.038510: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n",
      "Num GPUs Available:  1\n",
      "['10', '40', '50']\n",
      "Found 6851 files belonging to 3 classes.\n",
      "Using 5481 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742570564.472626    5577 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6851 files belonging to 3 classes.\n",
      "Using 1370 files for validation.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# Imports nécessaires pour construire un modèle LeNet \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Rescaling, BatchNormalization\n",
    "\n",
    "# Pour importer le datasets\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# Pour la compilation du modèle\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Pour visualiser les performances\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "\n",
    "print(tf.__version__)\n",
    "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
    "tf.keras.backend.clear_session()\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Chargement du dataset\n",
    "dir_name = \"/mnt/c/Users/karim/rakuten/images/data_clean/images_deep/sample\"\n",
    "#dir_name = \"/mnt/c/Users/karim/rakuten/images/data_clean/images_deep/train\"\n",
    "img_size = (224, 224)  # Taille cible\n",
    "batch_size = 128\n",
    "class_names = sorted(os.listdir(dir_name))\n",
    "nb_class = len(class_names)\n",
    "\n",
    "print(class_names)\n",
    "\n",
    "train_ds = image_dataset_from_directory(\n",
    "    dir_name,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    subset=\"training\",\n",
    "    validation_split=0.2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    dir_name,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    subset=\"validation\",\n",
    "    validation_split=0.2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Ajout d'optimisation : mise en cache et préchargement\n",
    "#train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "#val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf3aa2-a4e4-42e3-bc5d-d01bdf591987",
   "metadata": {},
   "source": [
    "## Version custom de LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951734a6-e63a-44de-ad43-54d91ed6d903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742570601.690867    5933 cuda_dnn.cc:529] Loaded cuDNN version 90800\n",
      "2025-03-21 16:23:24.814771: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:382] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 179ms/step - accuracy: 0.3739 - loss: 1.3557 - val_accuracy: 0.3818 - val_loss: 1.0533 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.4347 - loss: 1.0530 - val_accuracy: 0.4511 - val_loss: 1.0451 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.4848 - loss: 1.0123 - val_accuracy: 0.4504 - val_loss: 1.0414 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.4892 - loss: 1.0035 - val_accuracy: 0.5328 - val_loss: 0.9654 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 0.5393 - loss: 0.9578 - val_accuracy: 0.5679 - val_loss: 0.9045 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 0.5815 - loss: 0.8938 - val_accuracy: 0.5416 - val_loss: 0.9723 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 261ms/step - accuracy: 0.5759 - loss: 0.9249 - val_accuracy: 0.5774 - val_loss: 0.8995 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 428ms/step - accuracy: 0.6102 - loss: 0.8560 - val_accuracy: 0.6066 - val_loss: 0.8562 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.6225 - loss: 0.8511 - val_accuracy: 0.5934 - val_loss: 0.8785 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 0.6090 - loss: 0.8610 - val_accuracy: 0.5883 - val_loss: 0.8816 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 0.6297 - loss: 0.8390 - val_accuracy: 0.6365 - val_loss: 0.8321 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 0.6399 - loss: 0.8142 - val_accuracy: 0.6219 - val_loss: 0.8324 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 0.6292 - loss: 0.8214 - val_accuracy: 0.6591 - val_loss: 0.8078 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.6429 - loss: 0.7993 - val_accuracy: 0.6270 - val_loss: 0.8437 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 0.6423 - loss: 0.8052 - val_accuracy: 0.6307 - val_loss: 0.8221 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.6197 - loss: 0.8279 - val_accuracy: 0.6336 - val_loss: 0.8132 - learning_rate: 0.0010\n",
      "Epoch 17/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 0.6455 - loss: 0.7860 - val_accuracy: 0.6693 - val_loss: 0.7956 - learning_rate: 5.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 0.6671 - loss: 0.7713 - val_accuracy: 0.6854 - val_loss: 0.7799 - learning_rate: 5.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m40/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.6711 - loss: 0.7661"
     ]
    }
   ],
   "source": [
    "###  Architecture du model ###\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "\n",
    "# Data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.2),\n",
    "    tf.keras.layers.RandomContrast(0.2),\n",
    "])\n",
    "\n",
    "# Extraction des caractéristqiues\n",
    "x = data_augmentation(inputs)\n",
    "# Normalisation \n",
    "x = Rescaling(1./255)(x) \n",
    "\n",
    "# Première couche de convolution\n",
    "x = Conv2D(\n",
    "    filters=30,                    \n",
    "    kernel_size=(5, 5),            \n",
    "    padding='valid',               \n",
    "    activation='relu',            \n",
    ")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2),)(x)\n",
    "x = Dropout(0,2)(x)\n",
    "\n",
    "# Deuxième couche de convolution\n",
    "x = Conv2D(\n",
    "    filters=16,                    \n",
    "    kernel_size=(5, 5),            \n",
    "    padding='valid',               \n",
    "    activation='relu',            \n",
    ")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2),)(x)\n",
    "x = Dropout(0,2)(x)\n",
    "\n",
    "# Applatissement \n",
    "x = Flatten()(x)\n",
    "\n",
    "# Couches dense pour la prédiction \n",
    "dense_128 = Dense(\n",
    "    units=128,\n",
    "    activation='relu',\n",
    ")\n",
    "\n",
    "# Couche de sortie\n",
    "outputs = Dense(\n",
    "    units=3,\n",
    "    activation='softmax',\n",
    ")(x)\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(\n",
    "                                    monitor=\"val_loss\",\n",
    "                                    patience=3,\n",
    "                                    min_delta=0.01,\n",
    "                                    factor=0.5, \n",
    "                                    cooldown=4)\n",
    "\n",
    "model_lenet = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model_lenet.compile(\n",
    "    loss='sparse_categorical_crossentropy',  \n",
    "    optimizer=\"adam\",                 \n",
    "    metrics=['accuracy'])             \n",
    "\n",
    "model_lenet_history = model_lenet.fit(train_ds,           # données\n",
    "                           validation_data=val_ds,\n",
    "                           epochs=30,\n",
    "                           callbacks=[reduce_learning_rate])             # taille des batchs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d994c3-7f05-46ea-8c10-70b7c06aa5df",
   "metadata": {},
   "source": [
    "## Analyse de l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69963274-edda-436b-8c5b-918476e8e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(model_history, model_name):\n",
    "    # Récupérer les données d'entraînement et de validation\n",
    "    train_loss = model_history.history[\"loss\"]\n",
    "    val_loss = model_history.history[\"val_loss\"]\n",
    "    train_accuracy =  model_history.history[\"accuracy\"]\n",
    "    val_accuracy = model_history.history[\"val_accuracy\"]\n",
    "    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    \n",
    "    # Tracer la perte\n",
    "    plt.subplot(121)\n",
    "    plt.plot(train_loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.title(model_name + \": Perte d'entraînement et de validation\")\n",
    "    plt.ylabel('Perte ')\n",
    "    plt.xlabel('Époque')\n",
    "    plt.legend(['Entraînement', 'Validation'], loc='best')\n",
    "    \n",
    "    # Tracer l'erreur absolue moyenne (MAE)\n",
    "    plt.subplot(122)\n",
    "    plt.plot(train_accuracy)\n",
    "    plt.plot(val_accuracy)\n",
    "    plt.title(model_name+': Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Époque')\n",
    "    plt.legend(['Entraînement', 'Validation'], loc='best')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# modèle LeNet\n",
    "val_loss, val_accuracy = model_lenet.evaluate(val_ds)\n",
    "print(f\"Précision de validation finale: {val_accuracy:.4f}\")\n",
    "display_results(model_lenet_history, \"LeNet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab9f4a-5132-444a-b23f-dde239cbb64b",
   "metadata": {},
   "source": [
    "## Analyse de la performance par classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c96a90e-c084-4653-a111-fd2f90e34159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Générer des prédictions sur l'ensemble de validation\n",
    "# Prévoir les classes pour tous les échantillons de validation\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "# Récupérer toutes les étiquettes et prédictions\n",
    "for images, true_labels in val_ds:\n",
    "    pred = model.predict(images)\n",
    "    pred_classes = np.argmax(pred, axis=1)\n",
    "    \n",
    "    predictions.extend(pred_classes)\n",
    "    labels.extend(true_labels.numpy())\n",
    "\n",
    "# Convertir en arrays numpy\n",
    "predictions = np.array(predictions)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 2. Créer et afficher la matrice de confusion\n",
    "cm = confusion_matrix(labels, predictions)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Classe 0', 'Classe 1', 'Classe 2'],\n",
    "            yticklabels=['Classe 0', 'Classe 1', 'Classe 2'])\n",
    "plt.xlabel('Prédiction')\n",
    "plt.ylabel('Réalité')\n",
    "plt.title('Matrice de confusion')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
