{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de437451-7038-4e32-896f-89cbdf56c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Chargement et prétraitement d'une image depuis son chemin\n",
    "def load_and_preprocess_image(filepath):\n",
    "    image = tf.io.read_file(filepath)\n",
    "    image = tf.image.decode_image(image, channels=3)\n",
    "    image.set_shape([None, None, 3])\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    \n",
    "    # Normalisation pour EfficientNetB0 ([-1, 1] si preprocess_input est utilisé)\n",
    "    from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "    image = preprocess_input(image)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Fusion des prédictions image + texte\n",
    "def late_fusion_predict(text_model, image_model, text_input, image_path, alpha=0.5):\n",
    "    # Texte → SVM\n",
    "    prob_text = text_model.predict([text_input])[0]\n",
    "\n",
    "    # Image → EfficientNet\n",
    "    image_tensor = load_and_preprocess_image(image_path)\n",
    "    image_tensor = tf.expand_dims(image_tensor, axis=0)  # Add batch dimension\n",
    "    prob_image = image_model.predict(image_tensor, verbose=0)[0]\n",
    "\n",
    "    # Fusion\n",
    "    prob_combined = alpha * prob_image + (1 - alpha) * prob_text\n",
    "    predicted_class = np.argmax(prob_combined)\n",
    "\n",
    "    return predicted_class, prob_combined\n",
    "\n",
    "# Évaluation globale\n",
    "def evaluate_fusion(text_model, image_model, X_text, image_paths, y_true, alpha=0.5):\n",
    "    y_pred = []\n",
    "    for text_input, image_path in zip(X_text, image_paths):\n",
    "        pred_class, _ = late_fusion_predict(text_model, image_model, text_input, image_path, alpha)\n",
    "        y_pred.append(pred_class)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return acc, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43a6d761-6655-4f6e-a36b-d57f2194314d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karim/SEP24_CDS_Rakuten/env/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 24 variables whereas the saved optimizer has 46 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import tensorflow as tf\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Chargement du modèle pour le texte\n",
    "text_model = joblib.load('pipeline_svc_model.joblib')\n",
    "image_model = tf.keras.models.load_model('../../models/EfficientNetB0/EfficientNetB0_model_finetuned_best.keras')\n",
    "# Chargement de l'encoder\n",
    "label_encoder = joblib.load('label_encoder.joblib')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9e52fa7-f423-4851-af93-06404931b76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10555 entries, 3763 to 10159\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Unnamed: 0     10555 non-null  int64 \n",
      " 1   productid      10555 non-null  int64 \n",
      " 2   imageid        10555 non-null  int64 \n",
      " 3   prdtypecode    10555 non-null  int64 \n",
      " 4   merged         10555 non-null  object\n",
      " 5   category_name  10555 non-null  object\n",
      " 6   img_path       10555 non-null  object\n",
      "dtypes: int64(4), object(3)\n",
      "memory usage: 659.7+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "df = pd.read_csv('../../data/processed/clean_dataset.csv')\n",
    "\n",
    "dir_name = \"/mnt/c/Users/karim/rakuten/images/data_clean/image_train\"\n",
    "\n",
    "# Ou prendre un pourcentage (ex: 10% du DataFrame)\n",
    "df_echantillon = df.sample(frac=0.2)\n",
    "df_echantillon[\"img_path\"] = df_echantillon.apply(lambda row: os.path.join(dir_name, f\"image_{row['imageid']}_product_{row['productid']}.jpg\"), axis=1)\n",
    "\n",
    "df_echantillon.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c77b0b8-c999-468a-9175-c5e7f5d820bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = df_echantillon[\"merged\"]\n",
    "image_paths = df_echantillon[\"img_path\"]\n",
    "y_true = label_encoder.transform(df_echantillon[\"prdtypecode\"])\n",
    "\n",
    "acc, predictions = evaluate_fusion(text_model, image_model, X_text, image_paths, y_true, alpha=0.3)\n",
    "print(f\"Accuracy late fusion: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96e060a-30bb-4d83-86f2-af5332e206ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.0 → Accuracy=0.0379\n"
     ]
    }
   ],
   "source": [
    "for alpha in np.linspace(0, 0.5, 3):\n",
    "    acc, _ = evaluate_fusion(text_model, image_model, X_text, image_paths, y_true, alpha)\n",
    "    print(f\"alpha={alpha:.1f} → Accuracy={acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
